{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ÍNDICE**\n",
    "1. TRATAMIENTO DE LOS EJEMPLOS DE ENTRENAMIENTO\n",
    "    1. Extracción de los datos ofrecidos\n",
    "    2. Selección de técnicas de Aprendizaje automático\n",
    "2. TRATAMIENTO DE LOS EJEMPLOS DE VALIDACIÓN\n",
    "3. PRUEBAS Y MEJORA DE LAS TÉCNICAS ANTERIORES\n",
    "3. ANÁLISIS DE HIPÓTESIS INICIALES\n",
    "    3.1 Parámetros y datos obtenidos\n",
    "    3.2 Clasificación óptima\n",
    "4. CONCLUSIONES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # TRATAMIENTO DE LOS EJEMPLOS DE ENTRENAMIENTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACCIÓN DE LOS DATOS OFRECIDOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ARCENE** es un proyecto científico dedicado a la detección del cancer de ovarios y prósrata y clasificación de potenciales pacientes según datos de espectrometría de masa de los mismos.\n",
    "El *Data Set* ARCENE,con el que vamos a trabajar en este proyecto, es uno de los cinco *Data Set* del \"desafío de detección de Características NIPS. 2003\"\n",
    "El *Data Set* se divide en tres a su vez:\n",
    "- *Training Set*: conjunto de ejemplos de entrenamiento para diseñar nuestro clasificador de conjuntos de personas con cáncer y sin cáncer\n",
    "- *Validation Set*: conjunto de ejemplos de validación que nos permite calcular el porcentaje de acierto de nuestro clasificador y evaluar las hipótesis y decisiones iniciales tomadas relativas a las técnicas de entrenamiento, parámetros usados para funciones y cualquier decisión que influya en la precisión de nuestro sistema de aprendizaje automático.\n",
    "- *Test Set*: Se ofrece como un conjunto de datos de entrada reales para los que nuestro clasificador tendría que predecir en qué conjunto se encasilla.\n",
    "\n",
    "Solo disponemos de las salidas esperadas, o variables*y*, del Training Set y Validation Set. Es por ello, que estos son los arhivos de los que nos valdremos para diseñar el sistema de aprendizaje automático.\n",
    "\n",
    "**ESTRUCTURA**\n",
    "\n",
    "De los 900 registros de pacientes o ejemplos de los que disoponemos, 100 son destinados a ser ejemplos de entrenamiento para nuestro clasificador, otros 100 corresponden con los ejemplos de validación y los 700 restantes son ejemplos para el test final.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada uno de los registros de personas están formados por 10.000 características o atributos, de las cuales 3000 se añadieron de forma adicional y no tienen valor para la predcción de nuestro sistema.\n",
    "\n",
    "Las variables de entrada de los  ejemplos de entrenamiento se encuentran almacenadas en el archivo ***\"arcene_train.data\"*** y las etiquetas de salida esperada para cada uno de ellos las obtenemos de ***\"arcene_train.labels\".***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enlace a la descripción de las matrices con los datos de entrada [arcene.param](https://archive.ics.uci.edu/ml/machine-learning-databases/arcene/ARCENE/arcene.param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Paso 1- lectura de los data-set ofrecidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para la visualización de gráficos\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Procesamiento de datos\n",
    "import numpy as np\n",
    "from pandas.io.parsers import read_csv\n",
    "import scipy.optimize as opt\n",
    "from sklearn.preprocessing import PolynomialFeatures as pf\n",
    "from scipy.io import loadmat\n",
    "from sklearn import decomposition\n",
    "\n",
    "#Documentación del Código\n",
    "#para la traducción del código latex\n",
    "#from IPython.display import display, Math\n",
    "#import pdb; pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadMatrix(file):\n",
    "    return np.loadtxt(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lectura_Train_Set():\n",
    "    X = loadMatrix('arcene_train.data')\n",
    "    y = loadMatrix('arcene_train.labels')\n",
    "    return (X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  0.,  71.,   0., ...,   0.,   0., 524.],\n",
       "        [  0.,  41.,  82., ...,   0., 284., 423.],\n",
       "        [  0.,   0.,   1., ...,   0.,  34., 508.],\n",
       "        ...,\n",
       "        [  2.,  15.,  48., ...,   0.,   0., 453.],\n",
       "        [  8.,   0.,  38., ...,   0., 189., 403.],\n",
       "        [  0.,   0.,   0., ...,   0.,  10., 365.]]),\n",
       " array([ 1., -1.,  1.,  1., -1., -1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
       "         1., -1.,  1., -1., -1., -1., -1., -1., -1., -1.,  1., -1., -1.,\n",
       "         1., -1.,  1., -1.,  1.,  1.,  1., -1., -1.,  1., -1., -1.,  1.,\n",
       "        -1.,  1., -1., -1.,  1., -1., -1., -1., -1.,  1.,  1., -1.,  1.,\n",
       "        -1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1., -1.,\n",
       "        -1., -1., -1.,  1.,  1., -1.,  1., -1., -1.,  1., -1., -1.,  1.,\n",
       "        -1.,  1.,  1.,  1., -1.,  1.,  1., -1.,  1.,  1., -1., -1.,  1.,\n",
       "        -1.,  1.,  1., -1., -1., -1.,  1., -1.,  1.]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lectura_Train_Set()\n",
    "# A continuación se aprecia el tamaño de la matriz de datos de entrada y los valores de esta junto con los \n",
    "#labels correspondientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paso 2 - Selección de la técnica de aprendizaje automático"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la asignatura de Machine Learning hemos visto las siguientes técnicas aplicables al entrenamiento para la clasificación de unos datos de entrada en dos o varios grupos o clusters:\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1 Regresión lineal con varias variables**\n",
    "\n",
    "**2 Regresión logística**\n",
    "\n",
    "    1 Redes Neuronales\n",
    "\n",
    "    2 Support Vector Machines(SVM)\n",
    "\n",
    "    3 Clustering(k-means algorithms) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para decidir qué técnica usar nos puede ayudar representar los datos de entrada para observar gráficamente su distribución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grafic_train_examples_representation(X, y):\n",
    "    pos = np.where(y == 1)\n",
    "    pos0 = np.where(y == 0)\n",
    "    plt.scatter(X[pos,0], X[pos,1], marker='+', c='k')\n",
    "    plt.scatter(X[pos0,0], X[pos0,1], marker='o', c='g')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X, y = lectura_Train_Set()\n",
    "grafic_train_examples_representation(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar a simple vista, los datos no se distribuyen de forma lineal, descartando así la regresión lineal para su clasificación.\n",
    "Regresión logística nos será útil y la aplicaremos como técnica para entrenar una red neuronal y para calcular los support vector Machines de nuestro problema . Esto nos permitirá comparar ambas técnicas y analizar el porcentaje de acierto respecto a los ejemplos de entrenamiento de un mecanismo y de otro.\n",
    "\n",
    "Por otro lado clustering se aplica cuando disponemos de los datos de entrada ***X*** pero no de los datos de salida ***y*** para el entrenamiento.Este no es el caso de nuestro proyecto, de modo que tampoco necesitamos aplicar una técnica de entrenamiento no supervisado como la anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redes Neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aprender los valores de θ óptimos , primero les damos unos valores aleatorios de partida en la función ***\"pesos aleatorios\"*** y si aplicamos la función optimize de sklearn dentro de la función ***\"parametros\"***. En la función ***\"bestRegresion\"** iteramos sobre la invocación de la función ***\"parametros\"***, minimizando el error J(θ).\n",
    "\n",
    "Los θ que den lugar a ese valor óptimo del coste, serán los usados para predecir los ejemplos seleccionados para el test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pesosAleatorios(L_in, L_out):\n",
    "    epsilon = 6**(0.5) / (L_in + L_out)\n",
    "    return np.random.rand(L_in, L_out + 1) * 2 * epsilon - epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parametros(params, input_size, hidden_size, num_labels, X, y, reg):\n",
    "    result = opt.minimize(fun=backprop, x0=params,\n",
    "                         args=(input_size, hidden_size,\n",
    "                         num_labels, X, y, reg),\n",
    "                         method='TNC', jac=True,\n",
    "                         options={'maxiter':70})\n",
    "    return result.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones auxiliares para el backprop de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveMatrix(file, X):\n",
    "    return np.savetxt(file, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide(z):\n",
    "    return 1 / (1 + np.exp(-1*z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivadaSigmoide(z):\n",
    "    return sigmoide(z)*(1-sigmoide(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Retropropagación para calcular el coste y gradiente en una red neuronal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(params_rn, num_entradas, num_ocultas , num_etiquetas , X, y, reg):\n",
    "    \n",
    "    theta1 = np.reshape(params_rn[:num_ocultas*(num_entradas + 1)], (num_ocultas, (num_entradas + 1)))\n",
    "    theta2 = np.reshape(params_rn[num_ocultas*(num_entradas + 1):], (num_etiquetas, (num_ocultas + 1)))\n",
    "    m = len(X)\n",
    "   #Input\n",
    "    ones_columns_input = np.array(np.ones(m))\n",
    "    a1 = np.insert(X, 0,ones_columns_input, axis = 1)\n",
    "    \n",
    "    #hidden_layer\n",
    "    z2 = np.dot(theta1, a1.transpose())\n",
    "    a2 = sigmoide(z2)\n",
    "    one_columns_hidden = np.array(np.ones(m))\n",
    "    a2 = np.insert(a2, 0, one_columns_hidden, axis = 0)\n",
    "                                  \n",
    "    #Output_layer\n",
    "    z3 = np.dot(theta2, a2)\n",
    "    h = sigmoide(z3)\n",
    "    y_converted = (np.ravel(y) + 1)/2\n",
    "    \n",
    "     #Cost\n",
    "    regulation = (reg/(2*m)) * (np.sum(theta1**2) + np.sum(theta2**2))\n",
    "    J = np.sum(-y_converted * np.log(h) - (1 - y_converted)*np.log(1 - h)) * (1/m)\n",
    "    J_regulated = J + regulation\n",
    "    \n",
    "    # Retro-Propagation\n",
    "    d3 = h - y_converted\n",
    "    z2 = np.insert(z2, 0, np.ones(m), axis = 0)\n",
    "    z2prima = derivadaSigmoide(z2)\n",
    "    d2 = (np.dot(theta2.transpose(), d3))*z2prima\n",
    "    \n",
    "    #Gradient\n",
    "    delta2 = np.dot(d3,a2.transpose())\n",
    "    delta1 = np.dot(d2, a1)\n",
    "                                  \n",
    "    #Regularization\n",
    "    \n",
    "    D1 = (delta1[1:,:]/m + theta1*reg/m).ravel()\n",
    "    D2 = (delta2/m + theta2*reg/m).ravel()\n",
    "    gradient = np.r_[D1, D2]\n",
    "    \n",
    "    return J_regulated, gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coste(params, X, num_entradas, num_ocultas, num_etiquetas):\n",
    "    theta1 = np.reshape(params[:num_ocultas*(num_entradas + 1)], (num_ocultas, (num_entradas + 1)))\n",
    "    theta2 = np.reshape(params[num_ocultas*(num_entradas + 1):], (num_etiquetas, (num_ocultas + 1)))\n",
    "    m = len(X)\n",
    "    #Input\n",
    "    ones_columns_input = np.array(np.ones(m))\n",
    "    a1 = np.insert(X, 0,ones_columns_input, axis = 1)\n",
    "    #hidden_layer\n",
    "    z2 = np.dot(theta1, a1.transpose())\n",
    "    a2 = sigmoide(z2)\n",
    "    one_columns_hidden = np.array(np.ones(m))\n",
    "    a2 = np.insert(a2, 0, one_columns_hidden, axis = 0)\n",
    "    #Output_layer\n",
    "    z3 = np.dot(theta2, a2)\n",
    "    return sigmoide(z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluacion(params, X, Y, num_entradas, num_ocultas, num_etiquetas):\n",
    "    h = coste(params,X,num_entradas,num_ocultas,num_etiquetas)\n",
    "    z = (np.ravel(h) >= 0.5)\n",
    "    Y = (np.ravel(Y) + 1)/2\n",
    "    z = map((lambda x,y: x == y), z, Y)\n",
    "    print (Y)\n",
    "    print (z)\n",
    "    return np.ravel(sum(z)/float(len(z))*100)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestRegresion(theta1, theta2, num_entradas, num_ocultas, num_etiquetas, X, y, bound_left, bound_right, iterations):\n",
    "    regs = np.linspace(bound_left, bound_right, num=iterations)\n",
    "    params = np.concatenate((np.ravel(theta1), np.ravel(theta2)))\n",
    "    sol = []\n",
    "    for i in regs:\n",
    "        gradiente = parametros(params, num_entradas, num_ocultas, num_etiquetas, X, y, i)\n",
    "        e = evaluacion(gradiente, X, y, num_entradas, num_ocultas, num_etiquetas)\n",
    "        sol += [(i, e)]\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratamiento de los ejemplos de Validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidation(params, num_entradas, num_ocultas, num_etiquetas):\n",
    "    X = loadMatrix('arcene_valid.data')\n",
    "    pca = decomposition.PCA(n_components=100)\n",
    "    pca.fit(X)\n",
    "    #X = pca.transform(X)\n",
    "    h = coste(params, X, num_entradas, num_ocultas, num_etiquetas)\n",
    "    z = (np.ravel(h) >= 0.5)\n",
    "    return np.ravel(sum(z)/float(len(z))*100)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    reg = 10\n",
    "    X, y = lectura_Train_Set()\n",
    "    #X = loadMatrix('arcene_train.data')\n",
    "    \n",
    "    pca = decomposition.PCA(n_components=100)\n",
    "    pca.fit(X)\n",
    "    #X = pca.transform(X)\n",
    "    \n",
    "    #y = loadMatrix('arcene_train.labels')\n",
    "    y = y.astype(int)\n",
    "    num_entradas = X.shape[1]\n",
    "    num_ocultas = 350\n",
    "    num_etiquetas = 1\n",
    "\n",
    "    theta1 = pesosAleatorios(num_ocultas, X.shape[1])\n",
    "    theta2 = pesosAleatorios(num_etiquetas, num_ocultas)\n",
    "    #saveMatrix('theta1.out', theta1)\n",
    "    #saveMatrix('theta2.out', theta2)\n",
    "\n",
    "    #theta1 = loadMatrix('theta1.out')\n",
    "    #theta2 = loadMatrix('theta2.out')\n",
    "    cvX = loadMatrix('arcene_valid.data')\n",
    "    cvY = loadMatrix('arcene_valid.labels').astype(int)\n",
    "    pca.fit(cvX)\n",
    "    #cvX = pca.transform(cvX)\n",
    "    #print bestRegresion(theta1, theta2, num_entradas, num_ocultas, num_etiquetas, X, y, 20, 40.0, 3)\n",
    "    \n",
    "    #coste, gradiente = backprop(np.concatenate((np.ravel(theta1), np.ravel(theta2))), num_entradas, num_ocultas, num_etiquetas, X, y, reg)\n",
    "\n",
    "    pesos = np.concatenate((np.ravel(theta1), np.ravel(theta2)))\n",
    "    for i in range(1, int(len(X)/2)):\n",
    "        print (i)\n",
    "        pesos = parametros(pesos, num_entradas, num_ocultas, num_etiquetas, X[0:i*2], y[0:i*2], reg)\n",
    "        Jt = backprop(pesos, num_entradas, num_ocultas, num_etiquetas, X[0:i*2], y[0:i*2], reg)[0]\n",
    "        Jcv = backprop(pesos, num_entradas, num_ocultas, num_etiquetas, cvX[0:i*2], cvY[0:i*2], reg)[0]\n",
    "        print (Jt, Jcv, Jt - Jcv)\n",
    "        \n",
    "    #print (evaluacion(pesos, X, y, num_entradas, num_ocultas, num_etiquetas))\n",
    "    #print (evaluacion(pesos, cvX, cvY, num_entradas, num_ocultas, num_etiquetas))\n",
    "    #saveMatrix('weights.out', pesos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.5780029110901537 1.143253554369093 -0.5652506432789393\n",
      "2\n",
      "0.36805777818939056 1.295006148892784 -0.9269483707033934\n",
      "3\n",
      "0.4153203061410908 0.7247571649978395 -0.3094368588567487\n",
      "4\n",
      "0.4920594713237434 0.7355728442892038 -0.24351337296546038\n",
      "5\n",
      "0.5336446686189835 0.6727604871815329 -0.1391158185625494\n",
      "6\n",
      "0.6111973915189183 0.7246767463276133 -0.11347935480869498\n",
      "7\n",
      "0.6191057127183074 0.6681457063674859 -0.04903999364917855\n",
      "8\n",
      "0.6142679790597468 0.6503253747001819 -0.036057395640435086\n",
      "9\n",
      "0.5608611141396813 0.6941489987306865 -0.13328788459100516\n",
      "10\n",
      "0.4891908489847704 0.7437434894381217 -0.25455264045335135\n",
      "11\n",
      "0.45530822997686926 0.7190431567755522 -0.26373492679868293\n",
      "12\n",
      "0.44607553464216265 0.6925868361159967 -0.24651130147383404\n",
      "13\n",
      "0.418862721882175 0.7044746406500668 -0.2856119187678918\n",
      "14\n",
      "0.3952992657046585 0.681371376367113 -0.28607211066245447\n",
      "15\n",
      "0.37625650252299053 0.6368156729803701 -0.2605591704573796\n",
      "16\n",
      "0.4263847030733175 0.623755896988466 -0.1973711939151485\n",
      "17\n",
      "0.4144250562021254 0.6515145547212849 -0.23708949851915956\n",
      "18\n",
      "0.4481197102305883 0.6047673135732886 -0.15664760334270034\n",
      "19\n",
      "0.41222916100478224 0.5987465705272921 -0.18651740952250984\n",
      "20\n",
      "0.41042222872351675 0.5824287907425263 -0.17200656201900955\n",
      "21\n",
      "0.37298015069243645 0.5787316028710078 -0.20575145217857133\n",
      "22\n",
      "0.361158890467684 0.5619751627709534 -0.20081627230326937\n",
      "23\n",
      "0.3650080923029212 0.560517525289222 -0.19550943298630086\n",
      "24\n",
      "0.3617227557282858 0.5928345370490051 -0.2311117813207193\n",
      "25\n",
      "0.3629839256427188 0.603418543273829 -0.24043461763111024\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANÁLISIS DE HIPÓTESIS INICIALES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como tenemos overfiting, la solución no es aplicar svm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay overfiting:\n",
    "tres posibles soluciones: aumentar ejemplos de entrenamiento-> no porque los set ya nos los dan determinados\n",
    "reducir numero de features: no lo hacemos, porque no sabemos qué caraterísticas no afectan a la clasidicacion del cancer, no te lo dicen\n",
    "inviable porque hay 10.000 y tendríamos que quitar de 100 en 100-> riesgo de quitar variables buenas\n",
    "aumentar lambda > funciona, disminuye la diferencia entre Jcv y Jt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. reg = 10 -coger ej de train de dos en dos y nodos ocultos = 350 -> aciertos validacion = 86 %  y aciertos entrenamiento = 99%\n",
    "2. reg = 10 -coger ej de train de dos en dos y nodos ocultos = 250 -> aciertos validacion =  86%  y aciertos de entrenamiento = 99%\n",
    "3. reg = 10 -coger ej de train de dos en dos y nodos ocultos = 200 -> aciertos validacion =  %  y aciertos de entrenamiento = %\n",
    "4. reg = 10 -coger ej de train de dos en dos y nodos ocultos = 100 -> aciertos validacion = 84 %  y aciertos de entrenamiento = 99% ->peor tan poco no\n",
    "\n",
    "5. reg = 10 -coger ej de train de tres en tres, para todas iteraciones y nodos ocultos = 250 -> aciertos validacion = 79 %  y aciertos entrenamiento = 89 % -FATAL\n",
    "6. reg = 10 -coger ej de train todos,para todas iteraciones y nodos ocultos = 200 -> aciertos validacion =  84%  y aciertos de entrenamiento = 99%\n",
    "7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
